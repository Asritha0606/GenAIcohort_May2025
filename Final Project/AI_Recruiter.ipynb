{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIgxKBokEWKCKsvtShc4SQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asritha0606/GenAIcohort_May2025/blob/main/AI_Recruiter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task\n",
        "\n",
        "\n",
        "Create a Multiagent system as an AI Recruiter using crew ai that takes sample resumes and an HR query as input and returns the most matching resumes."
      ],
      "metadata": {
        "id": "AfRuN9JGluuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup and dependencies"
      ],
      "metadata": {
        "id": "aeh9_DEYl6cF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TQEiruqjFCF",
        "outputId": "e46cf9af-6771-4d90-e320-94d3b0682979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.52.4 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q crewai 'crewai[tools]' --progress-bar off together"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from crewai import Agent, Task, Crew"
      ],
      "metadata": {
        "id": "5OCzjQrrjGtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get('groq_api')\n",
        "os.environ[\"GROQ_API_KEY\"] = groq_api_key\n",
        "\n",
        "# hf_api_key = userdata.get('huggingfacehub')\n",
        "# os.environ[\"HUGGINGFACEHUB_API_KEY\"] = hf_api_key\n",
        "\n",
        "serper_api_key = userdata.get('serper_api')\n",
        "os.environ[\"SERPER_API_KEY\"] = serper_api_key\n",
        "\n",
        "gemini_api_key = userdata.get('Gemini_api_2')\n",
        "os.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n",
        "\n",
        "together_api_key = userdata.get('together_api')\n",
        "os.environ[\"TOGETHER_API_KEY\"] = together_api_key"
      ],
      "metadata": {
        "id": "2GnX7_yFjRas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import LLM\n",
        "llm = LLM(model=\"gemini/gemini-1.5-flash\", api_key=gemini_api_key)"
      ],
      "metadata": {
        "id": "kTgygw7sjTGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools"
      ],
      "metadata": {
        "id": "r7-ZIv4nmIop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai_tools import ScrapeWebsiteTool, SerperDevTool\n",
        "\n",
        "search_tool = SerperDevTool()\n",
        "scrape_tool = ScrapeWebsiteTool()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sDuhxlrjZfd",
        "outputId": "a1f20b79-cc1b-4d0f-c7db-531190336c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/fields.py:1093: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'required'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew\n",
        "# from langchain_groq.chat_models import ChatGroq # Corrected import\n",
        "import os"
      ],
      "metadata": {
        "id": "qX-pqSTpjcm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q crewai 'crewai[tools]' --progress-bar off together\n",
        "!pip install -q langchain groq langchain-groq langchain-community crewai_tools\n",
        "\n",
        "from crewai import Agent, Task, Crew, LLM\n",
        "from crewai_tools import (\n",
        "  FileReadTool,\n",
        "  ScrapeWebsiteTool,\n",
        "  MDXSearchTool,\n",
        "  SerperDevTool\n",
        ")\n",
        "\n",
        "\n",
        "try:\n",
        "    search_tool = SerperDevTool(api_key=serper_api_key)\n",
        "    print(\"Serper Dev Tool instantiated successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to instantiate Serper Dev Tool: {e}\")\n",
        "\n",
        "try:\n",
        "    scrape_tool = ScrapeWebsiteTool()\n",
        "    print(\"Scrape Website Tool instantiated successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to instantiate Scrape Website Tool: {e}\")\n",
        "\n",
        "try:\n",
        "    # Assuming the file exists from previous steps or will be created later\n",
        "    read_resume = FileReadTool(file_path='/content/Asritha_Sai_Resume.md')\n",
        "    print(\"File Read Tool instantiated successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to instantiate File Read Tool: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYTQ4nUrjhE8",
        "outputId": "218a1b75-2f8b-492d-e918-d5116df630a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m122.9/130.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSerper Dev Tool instantiated successfully.\n",
            "Scrape Website Tool instantiated successfully.\n",
            "File Read Tool instantiated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read_resume = FileReadTool()"
      ],
      "metadata": {
        "id": "IeZoxU8sowYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agents"
      ],
      "metadata": {
        "id": "4dye1h4BmRCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resume_reader = Agent(\n",
        "    role='Resume Reader',\n",
        "    goal='Read and extract the raw text content from various resume files.',\n",
        "    backstory=\"\"\"As a dedicated Resume Reader, you are meticulous in your task of accessing\n",
        "    and extracting the complete textual information from resume documents.\n",
        "    Your primary function is to provide the raw data for analysis.\"\"\",\n",
        "    tools=[read_resume],\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    allow_delegation=False\n",
        ")\n",
        "\n",
        "resume_analyzer = Agent(\n",
        "    role='Resume Analyzer',\n",
        "    goal='Analyze resume content to extract key skills, experience, and qualifications relevant to a given HR query.',\n",
        "    backstory=\"\"\"Equipped with a keen eye for detail, you are the Resume Analyzer.\n",
        "    You process the raw resume text, identify and categorize critical information\n",
        "    like technical skills, work history, education, and achievements,\n",
        "    specifically focusing on criteria provided in the HR query.\"\"\",\n",
        "    tools=[search_tool],\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    allow_delegation=True\n",
        ")\n",
        "\n",
        "hiring_manager = Agent(\n",
        "    role='Hiring Manager',\n",
        "    goal='Compare analyzed resumes against the HR query and select the most matching candidates.',\n",
        "    backstory=\"\"\"As the final decision-maker, you are the Hiring Manager.\n",
        "    You take the analyzed resume data and the HR query, evaluate the fit,\n",
        "    and determine which candidates are the most suitable for the role.\"\"\",\n",
        "    tools=[search_tool],\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        "    allow_delegation=False\n",
        ")\n",
        "\n",
        "print(\"Agents defined and instantiated successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98YwQGvTjqp_",
        "outputId": "64981431-17db-458d-f279-99ef7e833b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agents defined and instantiated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks"
      ],
      "metadata": {
        "id": "F0-cn5s2mZgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "read_resume_task = Task(\n",
        "    description=(\n",
        "        \"Read the content of the resume file located at {resume_file_path}.\"\n",
        "        \"Extract the complete raw text from the file.\"\n",
        "    ),\n",
        "    expected_output=\"A string containing the full text content of the resume file.\",\n",
        "    agent=resume_reader,\n",
        "    tools=[read_resume]\n",
        ")\n",
        "\n",
        "analyze_resume_task = Task(\n",
        "    description=(\n",
        "        \"Analyze the following resume text:\\n\\n\"\n",
        "        \"{resume_text}\\n\\n\"\n",
        "        \"Based on the HR query: '{hr_query}', extract and summarize the candidate's \"\n",
        "        \"key skills, work experience, education, and qualifications that are directly \"\n",
        "        \"relevant to the query. Identify strengths and weaknesses relative to the query.\"\n",
        "    ),\n",
        "    expected_output=(\n",
        "        \"A structured summary of the resume highlighting relevant skills, experience, \"\n",
        "        \"education, and qualifications based on the HR query. \"\n",
        "        \"Include a brief assessment of the candidate's fit for the role.\"\n",
        "    ),\n",
        "    agent=resume_analyzer,\n",
        "    context=[read_resume_task],\n",
        "    tools=[search_tool]\n",
        ")\n",
        "\n",
        "select_candidates_task = Task(\n",
        "    description=(\n",
        "        \"Compare the analyses of the following resumes against the HR query: '{hr_query}'.\\n\\n\"\n",
        "        \"Resume Analyses:\\n{resume_analyses}\\n\\n\"\n",
        "        \"Identify the top candidates based on the relevance of their skills, \"\n",
        "        \"experience, and qualifications as summarized in the analyses, compared to the HR query. \"\n",
        "        \"Provide a ranked list of the most matching candidates with a brief justification for each.\"\n",
        "    ),\n",
        "    expected_output=(\n",
        "        \"A ranked list of the most suitable candidates from the provided analyses, \"\n",
        "        \"including their name or resume identifier and a brief justification for their ranking \"\n",
        "        \"based on the HR query and the analysis.\"\n",
        "    ),\n",
        "    agent=hiring_manager,\n",
        "    context=[analyze_resume_task],\n",
        "    tools=[search_tool]\n",
        ")\n",
        "\n",
        "print(\"Tasks defined successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaQbUy8Ljtiu",
        "outputId": "7b458844-24b5-4882-931f-69b347e4ee62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tasks defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks for each resume and starting of Crew\n"
      ],
      "metadata": {
        "id": "_qwlaK0K1bt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the input HR query\n",
        "hr_query = \"We are looking for a data scientist with strong Python and machine learning skills, experience with cloud platforms like AWS or GCP, and a background in statistical modeling.\"\n",
        "\n",
        "# 2. Define the list of resume file paths\n",
        "\n",
        "resume_files = [\"/content/data/resume1.txt\", \"/content/data/resume2.txt\", \"/content/data/resume3.txt\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "k6_M5lzK1Z-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "read_tasks = [Task(\n",
        "    description=(\n",
        "        \"Read the content of the resume file located at {{ resume_file_path }}.\"\n",
        "        \"Extract the complete raw text from the file.\"\n",
        "    ),\n",
        "    expected_output=f\"A string containing the full text content of the resume file {{ resume_file_path }}.\",\n",
        "    agent=resume_reader,\n",
        "    tools=[read_resume],\n",
        "    # Pass the file path as an input parameter to the task\n",
        "    inputs={\"resume_file_path\": file_path},\n",
        "    task_id=f\"read_{i}\" # Add a unique ID for context\n",
        ") for i, file_path in enumerate(resume_files)]\n",
        "\n",
        "analyze_tasks = [Task(\n",
        "    description=(\n",
        "        f\"Analyze the following resume text from {resume_files[i]}:\\n\\n\"\n",
        "        \"{{ read_\" + str(i) + \".output }}\\n\\n\" # Use Jinja2 syntax to reference the output of the corresponding read task by task_id and '.output'\n",
        "        f\"Based on the HR query: '{hr_query}', extract and summarize the candidate's \"\n",
        "        \"key skills, work experience, education, and qualifications that are directly \"\n",
        "        \"relevant to the query. Identify strengths and weaknesses relative to the query.\"\n",
        "        f\"Label this analysis clearly as 'Analysis for {resume_files[i]}'.\"\n",
        "    ),\n",
        "    expected_output=(\n",
        "        f\"A structured summary of the resume from {resume_files[i]} highlighting relevant skills, experience, \"\n",
        "        \"education, and qualifications based on the HR query. \"\n",
        "        \"Include a brief assessment of the candidate's fit for the role.\"\n",
        "    ),\n",
        "    agent=resume_analyzer,\n",
        "    context=[read_tasks[i]], # Depends on the corresponding read task\n",
        "    tools=[search_tool],\n",
        "    task_id=f\"analyze_{i}\" # Add a unique ID for context\n",
        ") for i in range(len(resume_files))]\n",
        "\n",
        "# The final selection task depends on ALL analyze tasks\n",
        "select_candidates_task.context = analyze_tasks\n",
        "select_candidates_task.description = (\n",
        "    f\"Compare the analyses of the following {len(resume_files)} resumes against the HR query: '{hr_query}'.\\n\\n\"\n",
        "    \"Resume Analyses:\\n\"\n",
        "    # Use Jinja2 syntax to collect output from all analyze tasks in the context\n",
        "    + \"\\n\".join([f\"{{{{ analyze_\" + str(i) + \".output }}}}\" for i in range(len(resume_files))]) +\n",
        "    \"\\n\\n\"\n",
        "    \"Identify the top candidates based on the relevance of their skills, \"\n",
        "    \"experience, and qualifications as summarized in the analyses, compared to the HR query. \"\n",
        "    \"Provide a ranked list of the most matching candidates with a brief justification for each.\"\n",
        "    \"Output format should be a clear list, e.g., '1. Candidate A (Justification), 2. Candidate B (Justification)...'\"\n",
        ")\n",
        "\n",
        "\n",
        "# Combine all tasks into a single list for the crew, maintaining order for sequential execution\n",
        "all_tasks = read_tasks + analyze_tasks + [select_candidates_task]\n",
        "\n",
        "\n",
        "# 3. Instantiate the Crew\n",
        "recruiter_crew = Crew(\n",
        "    agents=[resume_reader, resume_analyzer, hiring_manager],\n",
        "    tasks=all_tasks,\n",
        "    verbose=False # Fix: Set verbose to True\n",
        ")\n",
        "\n",
        "# 4. Execute the crew\n",
        "crew_result = recruiter_crew.kickoff(inputs={'hr_query': hr_query})\n",
        "\n",
        "\n",
        "print(\"\\n## Crew Execution Complete ##\")\n",
        "print(\"Crew Result:\")\n",
        "print(crew_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrG9bXMljwEr",
        "outputId": "21ee2c64-b203-48f0-9777-f658330f42fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## Crew Execution Complete ##\n",
            "Crew Result:\n",
            "1. Candidate from resume2.txt (Strong Python skills, multiple ML models, strong academic background, and relevant projects. While lacking explicit cloud platform experience, their AI/ML focused internships and projects demonstrate practical application and potential. Further questioning during the interview can assess their statistical modeling experience and cloud platform knowledge.)  2. Candidate from resume3.txt (Similar strong Python and ML skills to candidate 2, with experience in diverse ML subfields. However, also lacks explicit cloud platform experience and detailed statistical modeling background.  Further probing during the interview will be necessary.) 3.  Candidate from resume1.txt (No information provided for this candidate, hence no ranking possible.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "Markdown(str(crew_result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "2jSb9XdolJqX",
        "outputId": "5563a17e-45b6-40f2-fb96-a7125d653d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "1. Candidate from resume2.txt (Strong Python skills, multiple ML models, strong academic background, and relevant projects. While lacking explicit cloud platform experience, their AI/ML focused internships and projects demonstrate practical application and potential. Further questioning during the interview can assess their statistical modeling experience and cloud platform knowledge.)  2. Candidate from resume3.txt (Similar strong Python and ML skills to candidate 2, with experience in diverse ML subfields. However, also lacks explicit cloud platform experience and detailed statistical modeling background.  Further probing during the interview will be necessary.) 3.  Candidate from resume1.txt (No information provided for this candidate, hence no ranking possible.)"
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio\n"
      ],
      "metadata": {
        "id": "PdZPmS-MqG9-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12fa3cec"
      },
      "source": [
        "!pip install -q gradio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "365a188e"
      },
      "source": [
        "import gradio as gr\n",
        "\n",
        "def run_ai_recruiter(hr_query, files):\n",
        "    read_tasks, analyze_tasks = [], []\n",
        "\n",
        "    # Dynamic resume read + analyze tasks\n",
        "    for i, file_obj in enumerate(files):\n",
        "        resume_path = file_obj.name\n",
        "        task_id = f\"resume_{i}\"\n",
        "\n",
        "        read_task = Task(\n",
        "            description=f\"Extract resume content from: {resume_path}\",\n",
        "            expected_output=\"Full resume text\",\n",
        "            agent=resume_reader,\n",
        "            tools=[read_resume],\n",
        "            inputs={\"resume_file_path\": resume_path},\n",
        "            task_id=f\"read_{i}\"\n",
        "        )\n",
        "\n",
        "        analyze_task = Task(\n",
        "            description=(\n",
        "                f\"Analyze resume for: {hr_query}\\n\\n\"\n",
        "                f\"{{{{ read_{i}.output }}}}\"\n",
        "            ),\n",
        "            expected_output=\"Structured analysis of the resume\",\n",
        "            agent=resume_analyzer,\n",
        "            tools=[search_tool],\n",
        "            context=[read_task],\n",
        "            task_id=f\"analyze_{i}\"\n",
        "        )\n",
        "\n",
        "        read_tasks.append(read_task)\n",
        "        analyze_tasks.append(analyze_task)\n",
        "\n",
        "    # Final task: Hiring manager selects best\n",
        "    select_task = Task(\n",
        "        description=(\n",
        "            f\"Compare resumes for: {hr_query}\\n\\n\" +\n",
        "            \"\\n\".join([f\"{{{{ analyze_{i}.output }}}}\" for i in range(len(files))])\n",
        "        ),\n",
        "        expected_output=\"Ranked candidate list with justifications.\",\n",
        "        agent=hiring_manager,\n",
        "        tools=[search_tool],\n",
        "        context=analyze_tasks\n",
        "    )\n",
        "\n",
        "    all_tasks = read_tasks + analyze_tasks + [select_task]\n",
        "\n",
        "    crew = Crew(\n",
        "        agents=[resume_reader, resume_analyzer, hiring_manager],\n",
        "        tasks=all_tasks,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    result = crew.kickoff(inputs={\"hr_query\": hr_query})\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(\n",
        "    fn=run_ai_recruiter,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"HR Query\", placeholder=\"Looking for a Python + AWS data scientist\"),\n",
        "        gr.File(label=\"Upload Resume .txt files\", file_types=['.txt'], file_count=\"multiple\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Matched Candidates\"),\n",
        "    title=\"🧠 AI Recruiter\",\n",
        "    description=\"Multi-Agent Resume Matcher with CrewAI\"\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "EVaYDzNxqamI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
